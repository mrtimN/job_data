{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "30d0f432-9e0d-4940-8b96-ad8b5d206202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install curl_cffi\n",
    "import pandas as pd\n",
    "import plotly as pl\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from curl_cffi import requests as cureq\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "url = 'data/combined_job_offers.csv'\n",
    "url2 = 'data/combined_job_offers3.csv'\n",
    "url_scraped = 'data/scraped_data.csv'\n",
    "\n",
    "# reading the 2 base datasets\n",
    "df1 = pd.read_csv(url)\n",
    "df2 = pd.read_csv(url2)\n",
    "\n",
    "df3 = pd.concat([df1,df2],axis=0)\n",
    "\n",
    "#reading the dataset, scraped from stepstone\n",
    "df_scraped = pd.read_csv(url_scraped)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4f9bd5c2-b2eb-4d2b-b44c-c79a5042f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the base dataset\n",
    "# 1 drop columns df3\n",
    "df3.drop(columns =['repost_date', 'email', 'job_desc'], inplace=True)\n",
    "\n",
    "# 2 Renaming 'link'\n",
    "df3.rename(columns={'link': 'source'}, inplace=True)\n",
    "\n",
    "# 3 replace all links with LinkedIn\n",
    "# apply lambda for each cell replace all string\n",
    "df3['source'] = df3['source'].apply(lambda x: 'LinkedIn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a4303ef9-16c0-4162-a575-437d1317dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from scraped dataset\n",
    "df_scraped.drop_duplicates(subset=['job_title', 'company_name', 'post_date'], keep='last', inplace=True)\n",
    "\n",
    "# combine scraped dataset with base dataset\n",
    "df_combined = pd.concat([df3,df_scraped],axis=0)\n",
    "\n",
    "# apply wrangling the job_levels\n",
    "df_combined['job_level'] = df_combined.apply(replace_nan_with_job_level, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9f838e0c-48d9-4b2a-ac18-a56804331d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stepstone(job_title, page, language, worktime, sector):\n",
    "    url = f'https://www.stepstone.de/jobs/{worktime}/{job_title}/in-berlin?radius=30&whereType=autosuggest&page={page}action=facet_selected%3bworktypes%3b80001&fdl={language}&se={sector}&wci=419239&sort=1&action=sort_relevance'\n",
    "    \n",
    "    print('---------------------------------')\n",
    "    print(url)\n",
    "    print('---------------------------------')\n",
    "    \n",
    "    response = cureq.get(url, impersonate='chrome')\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def enrich_dataset(job_title, page, language, worktime, sector, dataframe):\n",
    "    df_base = dataframe\n",
    "    df_enrich = dataframe.drop(dataframe.index)\n",
    "    \n",
    "    # scrape the initial result via scrape function\n",
    "    scrape_result = scrape_stepstone(job_title, page, language, worktime, sector)\n",
    "    \n",
    "    # get all job listings\n",
    "    job_offer_list = scrape_result.find_all(['article'], attrs={'class': 'res-1p8f8en'})\n",
    "\n",
    "    # set language, job_type, sector, source, search term according to search parameters\n",
    "    job_type_list = []\n",
    "    language_list = []\n",
    "    sector_list = []\n",
    "    source_list = []\n",
    "    search_term_list = []\n",
    "    for job_offer in job_offer_list:\n",
    "        job_type_list.append((lambda worktime: 'Full-time' if worktime == 'vollzeit' else 'Part-time' if worktime == 'teilzeit' else 'Unknown')(worktime))\n",
    "        language_list.append(language)\n",
    "        sector_list.append((lambda sector: 'IT Services and IT Consulting' if sector == '21000' else 'Business Consulting and Services' if sector == '23000' else 'Retail' if sector == '15000' else 'Finance' if sector == '19001' or sector == '19002' else 'Unknown')(sector))\n",
    "        source_list.append('stepstone')\n",
    "        search_term_list.append(job_title.replace('-', ' '))\n",
    "        \n",
    "    # scrape job titles\n",
    "    job_title_list = []\n",
    "    for job_offer in job_offer_list:\n",
    "        job_title_list.append(job_offer.find(['div'], attrs={'class': 'res-nehv70'}).get_text())\n",
    "    \n",
    "    # scrape company names\n",
    "    company_name_list = []\n",
    "    for job_offer in job_offer_list:\n",
    "        company_name_list.append(job_offer.find(['span'], attrs={'class': 'res-btchsq'}).get_text())\n",
    "    \n",
    "    # scrape post date\n",
    "    post_date_list = []\n",
    "    for job_offer in job_offer_list:\n",
    "        post_date_list.append(handle_date(job_offer.find(['time']).get_text()))\n",
    "    \n",
    "    # scrape salary - only possible with login\n",
    "    #salary_list = []\n",
    "    #for job_offer in job_offer_list:\n",
    "        #job_offer.find_all(['div'], attrs={'class': 'res-lgmafx'})[0]\n",
    "        #salary_list.append(job_offer.find(['span'], attrs={'class': 'res-1fad2gj'}).get_text())\n",
    "\n",
    "    # scrape and wrangle remote \n",
    "    job_remote_list = []\n",
    "    for job_offer in job_offer_list:\n",
    "        result = job_offer.find_all(['div'], attrs={'class': 'res-lgmafx'})\n",
    "        remote = result[0].find(['span'], attrs={'class': 'res-1qh7elo'})\n",
    "        if not remote:\n",
    "            job_remote_list.append('On-site')\n",
    "        else:\n",
    "            remote_text = remote.find(['span'], attrs={'class': 'res-btchsq'}).get_text()\n",
    "            if remote_text == 'Teilweise Home-Office':\n",
    "                job_remote_list.append('Hybrid')\n",
    "            elif remote_text == 'Nur Home-Office':\n",
    "                job_remote_list.append('Remote')\n",
    "            else:\n",
    "                job_remote_list.append('On-site')\n",
    "    \n",
    "    # fill the dataframe with the list values\n",
    "    df_enrich['job_title'] = job_title_list\n",
    "    df_enrich['company_name'] = company_name_list\n",
    "    df_enrich['post_date'] = post_date_list\n",
    "    #df_enrich[''] = number_of_employees\n",
    "    #df_enrich[''] = num_applicants\n",
    "    df_enrich['job_type'] = job_type_list\n",
    "    df_enrich['job_remote'] = job_remote_list\n",
    "    df_enrich['language'] = language_list    \n",
    "    #df_enrich[''] = salary_list\n",
    "    df_enrich['sector'] = sector_list\n",
    "    df_enrich['source'] = source_list\n",
    "    df_enrich['search_term'] = search_term_list\n",
    "\n",
    "    # combine base dataframe with the enriching data\n",
    "    df_combined = pd.concat([df_base, df_enrich],axis=0)\n",
    "    return df_combined\n",
    "\n",
    "# execute the scrape function in a nested loop to scrape a lot of data\n",
    "def scrape_a_lot(df_base):\n",
    "    # define the search parameters for the iterative scraping\n",
    "    search_terms = ['data analyst', 'data engineer', 'data scientist']\n",
    "    languages = ['en', 'de']\n",
    "    worktimes = ['vollzeit', 'teilzeit']\n",
    "    #21000 for 'it & internet', 23000 for 'bwl/business' 15000 for 'retail', 19001 for 'bank', 19002 for 'finance'  \n",
    "    sectors = ['21000', '23000', '15000', '19002']\n",
    "    df_temp = df_base.drop(df_base.index)\n",
    "    \n",
    "    # !!!BE CAREFUL - DO NOT UNCOMMENT AND EXECUTE!!!\n",
    "    # this will cause a lot of requests in short time and might result in an IP ban\n",
    "    #for language in languages:\n",
    "    #    for worktime in worktimes:\n",
    "    #        for search in search_terms:\n",
    "    #            for sector in sectors:\n",
    "    #                for i in range(1,3):\n",
    "    #                    df_temp = enrich_dataset(search, str(i), language, worktime, sector, df_base)\n",
    "    return df_temp\n",
    "\n",
    "# function to handle date format like \"1 week ago\" and convert it to datetime format\n",
    "def handle_date(stepstone_post_date):\n",
    "    sliced = stepstone_post_date.split(' ', 2)\n",
    "    date_number = int(sliced[1])\n",
    "    time_format = sliced[2]\n",
    "    date = datetime.now()\n",
    "    \n",
    "    match time_format:\n",
    "        case 'Stunden' | 'Stunde':\n",
    "            date = date - timedelta(hours=date_number)\n",
    "        case 'Tagen' | 'Tag':\n",
    "            date = date - timedelta(days=date_number)\n",
    "        case 'Wochen' | 'Woche':\n",
    "            date = date - timedelta(weeks=date_number)\n",
    "        case 'Monate' | 'Monat':\n",
    "            date = date - timedelta(months=date_number)\n",
    "        case _:\n",
    "           date = date\n",
    "    return date.strftime('%d-%m-%Y')\n",
    "\n",
    "# 3 job_type wrangling: extract_job_type for NaN with keywords and 'Unknown'\n",
    "def replace_nan_with_job_level(row):\n",
    "    keywords = ['Senior', 'Mid-Senior level', 'Associate', 'Entry', 'Entry level','Werkstudent', 'Junior', 'Consultant', 'Developer', 'Mid', 'Intern', 'Lead', 'Manager', 'Internship', 'Entry level','Director', 'Mid-Senior']\n",
    "    \n",
    "    if pd.isna(row['job_level']):\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', row['job_title'], re.IGNORECASE):\n",
    "                return keyword\n",
    "        return 'Unknown' \n",
    "    return row['job_level']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
