{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d0f432-9e0d-4940-8b96-ad8b5d206202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install curl_cffi\n",
    "import pandas as pd\n",
    "import plotly as pl\n",
    "from bs4 import BeautifulSoup\n",
    "from curl_cffi import requests as cureq\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "url = \"data/combined_job_offers.csv\"\n",
    "url2 = \"data/combined_job_offers3.csv\"\n",
    "\n",
    "df1 = pd.read_csv(url)\n",
    "df2 = pd.read_csv(url2)\n",
    "\n",
    "df3 = pd.concat([df1,df2],axis=0)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f838e0c-48d9-4b2a-ac18-a56804331d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stepstone(job_title, page, language, worktime, sector):\n",
    "    \n",
    "    lang_filter = f\"&action=facet_selected%3bdetectedLanguages%3bde&fdl={language}\" # must be 'de' or 'en'\n",
    "    worktime_filter = f\"&action=facet_selected%3bworktypes%3b8000{worktime}\" # must be '1' for fulltime or '2' for parttime\n",
    "    sector_filter = f\"&action=facet_selected%3bsectors%3b21000&se={sector}\" # 21000 for 'it & internet', 23000 for 'bwl/business' 15000 for 'retail', 19001 for 'bank', 19002 for 'finance'  \n",
    "    if worktime == \"1\":\n",
    "        test = \"vollzeit\"\n",
    "    elif worktime == \"2\":\n",
    "        test = \"teilzeit\"\n",
    "    # compare number of existing pages to user entries\n",
    "    #url = f\"https://www.stepstone.de/jobs/{job_title}/in-berlin?radius=10&page={page}{lang_filter}{worktime_filter}{worktime_filter}\" # job_title must be tile with '-' separator\n",
    "    \n",
    "    url = f\"https://www.stepstone.de/jobs/{test}/{job_title}/in-berlin?radius=10&action=facet_selected%3bworktypes%3b80001&fdl={language}&se={sector}\"\n",
    "    \n",
    "    response = cureq.get(url, impersonate='chrome')\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def handle_date(stepstone_post_date):\n",
    "    sliced = stepstone_post_date.split(\" \", 2)\n",
    "    date_number = int(sliced[1])\n",
    "    time_format = sliced[2]\n",
    "    date = datetime.now()\n",
    "    \n",
    "    match time_format:\n",
    "        case \"Stunden\" | \"Stunde\":\n",
    "            date = date - timedelta(hours=date_number)\n",
    "        case \"Tagen\" | \"Tag\":\n",
    "            date = date - timedelta(days=date_number)\n",
    "        case \"Wochen\" | \"Woche\":\n",
    "            date = date - timedelta(weeks=date_number)\n",
    "        case \"Monate\" | \"Monat\":\n",
    "            date = date - timedelta(months=date_number)\n",
    "        case _:\n",
    "           date = date\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a8214e-81a4-4569-893c-b7c6ce3537ea",
   "metadata": {},
   "outputs": [],
   "source": [
   "# 3 replace all links with LinkedIn\n",
    "# apply lambda for each cell replace all string\n",
    "df4 = df3_dropped\n",
    "df4['source'] = df4['source'].apply(lambda x: 'LinkedIn')"
    "# 1 drop columns df3\n",
    "df3_dropped = df3.drop(columns =['repost_date', 'email', 'job_desc'])\n"
    "# 2 Renaming 'link'\n",
    "df3_dropped.rename(columns={'link': 'source'}, inplace=True)\n"
    "# scrape for testing purpose\n",
    "test = scrape_stepstone('data-analyst', '2', 'de', '2', '21000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f1af0e-033b-4142-9328-a096556978eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "['Sopra Steria', 'HMS Analytical Software GmbH', 'HMS Analytical Software GmbH', 'Atruvia AG', 'FUNKE Works GmbH', 'inovex GmbH', 'Atruvia AG', 'ParshipMeet Group', 'inovex GmbH', 'Atruvia AG', 'BridgingIT GmbH', 'BridgingIT GmbH', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'ecratum GmbH', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'HMS Analytical Software GmbH', 'LIQID Investments GmbH']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all job listings\n",
    "itest = test.find_all(['article'], attrs={\"class\": 'res-1p8f8en'})\n",
    "\n",
    "# scrape job titles\n",
    "job_title_list = []\n",
    "for item in itest:\n",
    "    job_title_list.append(item.find(['div'], attrs={'class': 'res-nehv70'}).get_text())\n",
    "\n",
    "# scrape company names\n",
    "company_name_list = []\n",
    "for item in itest:\n",
    "    company_name_list.append(item.find(['span'], attrs={'class': 'res-btchsq'}).get_text())\n",
    "\n",
    "# scrape post date\n",
    "post_date_list = []\n",
    "for item in itest:\n",
    "    post_date_list.append(handle_date(item.find(['time']).get_text()))\n",
    "\n",
    "# scrape salary - only possible with login\n",
    "salary_list = []\n",
    "for item in itest:\n",
    "    item.find_all(['div'], attrs={'class': 'res-lgmafx'})[0]\n",
    "    #salary_list.append(item.find(['span'], attrs={'class': 'res-1fad2gj'}).get_text())\n",
    "\n",
    "# scrape remote \n",
    "remote_list = []\n",
    "for item in itest:\n",
    "    result = item.find_all(['div'], attrs={'class': 'res-lgmafx'})\n",
    "    remote = result[0].find(['span'], attrs={'class': 'res-1qh7elo'})\n",
    "    try:\n",
    "        remote_exists = remote.find(['span'], attrs={'class': 'res-btchsq'}).get_text()\n",
    "        remote_list.append(remote_exists)\n",
    "    except AttributeError:\n",
    "        remote_exits = \"on-site\"\n",
    "        remote_list.append(remote_exists)\n",
    "        \n",
    "    \n",
    "print(len(job_title_list))\n",
    "print(len(remote_list))\n",
    "print(company_name_list)\n",
    "remote_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e6754-0384-4e4c-95b6-4a90ab041ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
