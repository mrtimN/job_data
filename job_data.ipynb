{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d0f432-9e0d-4940-8b96-ad8b5d206202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install curl_cffi\n",
    "import pandas as pd\n",
    "import plotly as pl\n",
    "from bs4 import BeautifulSoup\n",
    "from curl_cffi import requests as cureq\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "url = \"data/combined_job_offers.csv\"\n",
    "url2 = \"data/combined_job_offers3.csv\"\n",
    "\n",
    "df1 = pd.read_csv(url)\n",
    "df2 = pd.read_csv(url2)\n",
    "\n",
    "df3 = pd.concat([df1,df2],axis=0)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f9bd5c2-b2eb-4d2b-b44c-c79a5042f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "# 1 dropping not needed columns\n",
    "df3.drop(columns =['repost_date', 'email', 'job_desc'], inplace=True)\n",
    "# 2 Renaming 'link'\n",
    "df3.rename(columns={'link': 'source'}, inplace=True)\n",
    "# 3 replace all links with LinkedIn\n",
    "# apply lambda for each cell replace all string\n",
    "df3['source'] = df3['source'].apply(lambda x: 'LinkedIn')# 1 drop columns df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f838e0c-48d9-4b2a-ac18-a56804331d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stepstone(job_title, page, language, worktime, sector):\n",
    "    \n",
    "    lang_filter = f\"&action=facet_selected%3bdetectedLanguages%3bde&fdl={language}\" # must be 'de' or 'en'\n",
    "    worktime_filter = f\"&action=facet_selected%3bworktypes%3b8000{worktime}\" # must be '1' for fulltime or '2' for parttime\n",
    "    sector_filter = f\"&action=facet_selected%3bsectors%3b21000&se={sector}\" # 21000 for 'it & internet', 23000 for 'bwl/business' 15000 for 'retail', 19001 for 'bank', 19002 for 'finance'  \n",
    "    if worktime == \"1\":\n",
    "        test = \"vollzeit\"\n",
    "    elif worktime == \"2\":\n",
    "        test = \"teilzeit\"\n",
    "    # compare number of existing pages to user entries\n",
    "    #url = f\"https://www.stepstone.de/jobs/{job_title}/in-berlin?radius=10&page={page}{lang_filter}{worktime_filter}{worktime_filter}\" # job_title must be tile with '-' separator\n",
    "    \n",
    "    url = f\"https://www.stepstone.de/jobs/{test}/{job_title}/in-berlin?radius=10&action=facet_selected%3bworktypes%3b80001&fdl={language}&se={sector}\"\n",
    "    \n",
    "    response = cureq.get(url, impersonate='chrome')\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def handle_date(stepstone_post_date):\n",
    "    sliced = stepstone_post_date.split(\" \", 2)\n",
    "    date_number = int(sliced[1])\n",
    "    time_format = sliced[2]\n",
    "    date = datetime.now()\n",
    "    \n",
    "    match time_format:\n",
    "        case \"Stunden\" | \"Stunde\":\n",
    "            date = date - timedelta(hours=date_number)\n",
    "        case \"Tagen\" | \"Tag\":\n",
    "            date = date - timedelta(days=date_number)\n",
    "        case \"Wochen\" | \"Woche\":\n",
    "            date = date - timedelta(weeks=date_number)\n",
    "        case \"Monate\" | \"Monat\":\n",
    "            date = date - timedelta(months=date_number)\n",
    "        case _:\n",
    "           date = date\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a8214e-81a4-4569-893c-b7c6ce3537ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape for testing purpose\n",
    "test = scrape_stepstone('data-analyst', '2', 'de', '2', '21000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f1af0e-033b-4142-9328-a096556978eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n",
      "['Sopra Steria', 'HMS Analytical Software GmbH', 'HMS Analytical Software GmbH', 'Atruvia AG', 'FUNKE Works GmbH', 'inovex GmbH', 'Atruvia AG', 'ParshipMeet Group', 'inovex GmbH', 'Atruvia AG', 'BridgingIT GmbH', 'BridgingIT GmbH', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'ecratum GmbH', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'NTT DATA Business Solutions AG', 'HMS Analytical Software GmbH', 'LIQID Investments GmbH']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office',\n",
       " 'Teilweise Home-Office']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all job listings\n",
    "itest = test.find_all(['article'], attrs={\"class\": 'res-1p8f8en'})\n",
    "\n",
    "# scrape job titles\n",
    "job_title_list = []\n",
    "for item in itest:\n",
    "    job_title_list.append(item.find(['div'], attrs={'class': 'res-nehv70'}).get_text())\n",
    "\n",
    "# scrape company names\n",
    "company_name_list = []\n",
    "for item in itest:\n",
    "    company_name_list.append(item.find(['span'], attrs={'class': 'res-btchsq'}).get_text())\n",
    "\n",
    "# scrape post date\n",
    "post_date_list = []\n",
    "for item in itest:\n",
    "    post_date_list.append(handle_date(item.find(['time']).get_text()))\n",
    "\n",
    "# scrape salary - only possible with login\n",
    "salary_list = []\n",
    "for item in itest:\n",
    "    item.find_all(['div'], attrs={'class': 'res-lgmafx'})[0]\n",
    "    #salary_list.append(item.find(['span'], attrs={'class': 'res-1fad2gj'}).get_text())\n",
    "\n",
    "# scrape remote \n",
    "remote_list = []\n",
    "for item in itest:\n",
    "    result = item.find_all(['div'], attrs={'class': 'res-lgmafx'})\n",
    "    remote = result[0].find(['span'], attrs={'class': 'res-1qh7elo'})\n",
    "    try:\n",
    "        remote_exists = remote.find(['span'], attrs={'class': 'res-btchsq'}).get_text()\n",
    "        remote_list.append(remote_exists)\n",
    "    except AttributeError:\n",
    "        remote_exits = \"on-site\"\n",
    "        remote_list.append(remote_exists)\n",
    "        \n",
    "    \n",
    "print(len(job_title_list))\n",
    "print(len(remote_list))\n",
    "print(company_name_list)\n",
    "remote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c6b83db-cf2e-483f-88b8-583a6f7657cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#1 stepstone wrangling: created stepstone_df \n",
    "column_dict = {\"job_title\": job_title_list, \"company_name\":company_name_list, \"post_date\":post_date_list, \"job_remote\": remote_list}\n",
    "stepstone_df = pd.DataFrame(column_dict)\n",
    "\n",
    "#2 stepstone wrangling: extract_job_type\n",
    "\n",
    "import re\n",
    "job_level = []\n",
    "def extract_job_type(title):\n",
    "    keywords = ['Senior', 'Werkstudent', 'Junior', 'Consultant', 'Developer', 'Mid', 'Intern', 'Lead', 'Manager', 'Internship', 'Entry level','Director', 'Mid-Senior']\n",
    "    for keyword in keywords:\n",
    "        if re.search (r'\\b' + re.escape(keyword) + r'\\b', title, re.IGNORECASE):\n",
    "            return keyword\n",
    "    return 'Unknown'\n",
    "\n",
    "stepstone_df['job_level'] = stepstone_df['job_title'].apply(extract_job_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dcf165cb-832b-4b49-8872-e8e06bc52a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 linkedin wrangling:extract_job_type for NaN with keywords and 'Unknown'\n",
    "import re\n",
    "df3['job_level']\n",
    "def replace_nan_with_keywords(row):\n",
    "    keywords = ['Senior', 'Mid-Senior level', 'Associate', 'Entry', 'Entry level','Werkstudent', 'Junior', 'Consultant', 'Developer', 'Mid', 'Intern', 'Lead', 'Manager', 'Internship', 'Entry level','Director', 'Mid-Senior']\n",
    "    \n",
    "    if pd.isna(row['job_level']):\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', row['job_title'], re.IGNORECASE):\n",
    "                return keyword\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return row['job_level']\n",
    "\n",
    "df3['job_level'] = df3.apply(replace_nan_with_keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b76be-e90a-4015-939d-1b138d47206a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9e75e20-dee9-41f4-95ce-36db27766b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Class: Micro-enterprises, Persons Employed: Up to 9\n"
     ]
    }
   ],
   "source": [
    "# Possible API for company size by employee count\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = 'https://www.destatis.de/EN/Themes/Economic-Sectors-Enterprises/Enterprises/Small-Sized-Enterprises-Medium-Sized-Enterprises/ExplanatorySME.html'\n",
    "\n",
    "# Send a request to fetch the HTML of the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Example of finding a table (assuming you're targeting specific tables)\n",
    "# You may need to adjust the selectors according to the actual structure\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# Depending on the structure of the tables, you may want to inspect manually and adjust\n",
    "# Assuming we're interested in the first table\n",
    "if tables:\n",
    "    table = tables[0]  # Or use tables[x] if not the first\n",
    "\n",
    "    # Extract table rows\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        # Extract the columns (elements) in each row\n",
    "        cols = row.find_all('td')\n",
    "        size_class = cols[0].get_text(strip=True) if len(cols) > 0 else ''\n",
    "        persons_employed = cols[1].get_text(strip=True) if len(cols) > 1 else ''\n",
    "        # annual_turnover = cols[2].get_text(strip=True) if len(cols) > 2 else ''\n",
    "        \n",
    "        # If looking for specific size classes\n",
    "        if size_class in [\"Micro-enterprises\", \"Small enterprises\", \"Medium-sized enterprises\"]:\n",
    "            print(f\"Size Class: {size_class}, Persons Employed: {persons_employed}\") #, Annual Turnover: {annual_turnover}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bae67-0869-48b0-8e67-16746ad6af6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03dfc5-7a4c-4823-8650-abe2b0ce2055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9292053a-d115-4eac-b317-6455587ef3f4",
   "metadata": {},
   "source": [
    "username = \"\"\n",
    "password = \"\"\n",
    "credentials = {\"username\": username, \"password\": password}\n",
    "response = cureq.get(url, data = credentials, impersonate='chrome')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
